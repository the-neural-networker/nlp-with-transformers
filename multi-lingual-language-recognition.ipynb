{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (/u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c954edb5f2f4c30a5d9165ea0d90a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"xtreme\", name=\"PAN-X.te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xtreme (/u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812d69be2f5345c398f852966d24da1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-68314036533271e4.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-8dbe0c17aec505e3.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-16c4e170f4c083d6.arrow\n",
      "Found cached dataset xtreme (/u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdeed2c8077421a8ce900f22f4575d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-021189a66c8ca2fc.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a6dd7522f39237eb.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-fb8462d1a7a7ec54.arrow\n",
      "Found cached dataset xtreme (/u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0803c59e80094efa875824739792c8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-6ce4719467e98050.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-ba0404565ea374a4.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-29467d2078f2faf4.arrow\n",
      "Found cached dataset xtreme (/u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ecbf5553d5473487cdad3afb46effc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-e14b50505509ca06.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-529925d4984531e4.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-ef60137063549caf.arrow\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict \n",
    "\n",
    "langs = [\"hi\", \"te\", \"ta\", \"en\"]\n",
    "fracs = [0.5709, 0.0777, 0.6360, 0.1067]\n",
    "fracs = [frac / sum(fracs) for frac in fracs]\n",
    "# return a DatasetDict if a key does not exist\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # load multilingual corpus \n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # shuffle and downsample each split according to spoken proportion\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle(seed=42)\n",
    "            .select(range(int(frac * ds[split].num_rows)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hi</th>\n",
       "      <th>te</th>\n",
       "      <th>ta</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Number of training examples</th>\n",
       "      <td>2051</td>\n",
       "      <td>55</td>\n",
       "      <td>6856</td>\n",
       "      <td>1533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hi  te    ta    en\n",
       "Number of training examples  2051  55  6856  1533"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs}, \n",
    "             index=[\"Number of training examples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['प्रेम', 'चोपड़ा', '-', 'गिरिधारीलाल']\n",
      "ner_tags: [1, 2, 0, 0]\n",
      "langs: ['hi', 'hi', 'hi', 'hi']\n"
     ]
    }
   ],
   "source": [
    "element = panx_ch[\"hi\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "ner_tags: Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)\n",
      "langs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "for key, value in panx_ch[\"hi\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)\n"
     ]
    }
   ],
   "source": [
    "tags = panx_ch[\"hi\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a5e82638fe2cec57.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a4af9bc9df139198.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-8fa5e26de647100a.arrow\n"
     ]
    }
   ],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "\n",
    "panx_hi = panx_ch[\"hi\"].map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>प्रेम</td>\n",
       "      <td>चोपड़ा</td>\n",
       "      <td>-</td>\n",
       "      <td>गिरिधारीलाल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1  2            3\n",
       "Tokens  प्रेम  चोपड़ा  -  गिरिधारीलाल\n",
       "Tags    B-PER   I-PER  O            O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_example = panx_hi[\"train\"][0]\n",
    "pd.DataFrame([hi_example[\"tokens\"], hi_example[\"ner_tags_str\"]], ['Tokens', 'Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PER</th>\n",
       "      <th>ORG</th>\n",
       "      <th>LOC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>945</td>\n",
       "      <td>753</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>177</td>\n",
       "      <td>140</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>158</td>\n",
       "      <td>151</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PER  ORG  LOC\n",
       "train       945  753  790\n",
       "validation  177  140  185\n",
       "test        158  151  176"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_hi.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1 \n",
    "\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whitespace is preserved by using xlmr which uses the SentencePiece tokenizer\n",
    "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(XLMRobertaForTokenClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # load model body\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # set up token classification head \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # load and initialize weights \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        # use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
    "        # apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # calculate losses \n",
    "        loss = None \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # return model output object \n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=tags.num_classes, id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'roberta.embeddings.position_ids', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3      4  5     6      7   8     9\n",
       "Tokens     <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\n",
       "Input IDs    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on a small sequence of known entities \n",
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 10\n",
      "Shape of outputs: torch.Size([1, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits \n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
    "print(F\"Shape of outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      9\n",
       "Tokens    <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York      !   </s>\n",
       "Tags    B-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  B-ORG"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # encode the sequence into IDs\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # get predictions as a distribution over 7 possible classes \n",
    "    outputs = model(input_ids)[0]\n",
    "    # take argmax to get most likely class per token \n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # convert to dataframe\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Texts for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, labels = hi_example[\"tokens\"], hi_example[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁प्रेम</td>\n",
       "      <td>▁चोपड़ा</td>\n",
       "      <td>▁-</td>\n",
       "      <td>▁गिरि</td>\n",
       "      <td>धारी</td>\n",
       "      <td>लाल</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1        2   3      4     5    6     7\n",
       "Tokens  <s>  ▁प्रेम  ▁चोपड़ा  ▁-  ▁गिरि  धारी  लाल  </s>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = xlmr_tokenizer(hi_example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁प्रेम</td>\n",
       "      <td>▁चोपड़ा</td>\n",
       "      <td>▁-</td>\n",
       "      <td>▁गिरि</td>\n",
       "      <td>धारी</td>\n",
       "      <td>लाल</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1        2   3      4     5    6     7\n",
       "Tokens     <s>  ▁प्रेम  ▁चोपड़ा  ▁-  ▁गिरि  धारी  लाल  </s>\n",
       "Word IDs  None       0        1   2      3     3    3  None"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to mask the subwords after the first subword\n",
    "word_ids = tokenized_input.word_ids() \n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁प्रेम</td>\n",
       "      <td>▁चोपड़ा</td>\n",
       "      <td>▁-</td>\n",
       "      <td>▁गिरि</td>\n",
       "      <td>धारी</td>\n",
       "      <td>लाल</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0       1        2   3      4     5     6     7\n",
       "Tokens       <s>  ▁प्रेम  ▁चोपड़ा  ▁-  ▁गिरि  धारी   लाल  </s>\n",
       "Word IDs    None       0        1   2      3     3     3  None\n",
       "Label IDs   -100       1        2   0      0  -100  -100  -100\n",
       "Labels     B-PER   I-PER        O   O   None  None  None  None"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_word_idx = None \n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in labels]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = [] \n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None \n",
    "        label_ids = [] \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-a268d1bbb148d385.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-b9fb3bcaedd13b7d.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.hi/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-7742a222018ea25d.arrow\n"
     ]
    }
   ],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])\n",
    "\n",
    "panx_hi_encoded = encode_panx_dataset(panx_ch[\"hi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], [] \n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], [] \n",
    "        for seq_idx in range(seq_len):\n",
    "            # ignore label ids = -100 \n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 24\n",
    "logging_steps = len(panx_hi_encoded[\"train\"]) // batch_size \n",
    "model_name = f\"{xlmr_model_name}-finetuned-panx-hi\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs, \n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1fa9971f534abeba83d4818b3f09cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login \n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score \n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification \n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/xlm-roberta-base-finetuned-panx-hi is already a clone of https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-hi. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer \n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args, data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                   train_dataset=panx_hi_encoded[\"train\"], eval_dataset=panx_hi_encoded[\"validation\"], tokenizer=xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129/129 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.430049</td>\n",
       "      <td>0.656420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.802337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>0.279025</td>\n",
       "      <td>0.829746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3e4167acbc4e62878e76074652465a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tokenizer.json:   0%|          | 1.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2d45faf2844592b0a31f1f0ed27881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-hi\n",
      "   b74918b..b7e49ba  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-hi/commit/b7e49ba67e9a0b3a867eaa0f9303b6d935554379'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁तेजी</td>\n",
       "      <td>▁बच्चन</td>\n",
       "      <td>▁से</td>\n",
       "      <td>▁अमिताभ</td>\n",
       "      <td>▁तथा</td>\n",
       "      <td>▁अज</td>\n",
       "      <td>िता</td>\n",
       "      <td>भ</td>\n",
       "      <td>▁दो</td>\n",
       "      <td>▁पुत्र</td>\n",
       "      <td>▁हुए</td>\n",
       "      <td>▁।</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1       2    3        4     5      6      7      8    9    \n",
       "Tokens  <s>  ▁तेजी  ▁बच्चन  ▁से  ▁अमिताभ  ▁तथा    ▁अज    िता      भ  ▁दो  \\\n",
       "Tags      O  B-PER   I-PER    O    B-PER     O  B-PER  I-PER  I-PER    O   \n",
       "\n",
       "            10    11  12    13  \n",
       "Tokens  ▁पुत्र  ▁हुए  ▁।  </s>  \n",
       "Tags         O     O   O     O  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hi = \"तेजी बच्चन से अमिताभ तथा अजिताभ दो पुत्र हुए ।\"\n",
    "tag_text(text_hi, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # convert dict of lists of list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    # pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        # pass data through model\n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "        # logit.size [batch_size, sequence_length, classes]\n",
    "        # predict class with largest logit value on classes axis \n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy() \n",
    "    \n",
    "    # calculate loss per token after flattening batch dimension with view \n",
    "    loss = cross_entropy(output.logits.view(-1, 7), labels.view(-1), reduction=\"none\") \n",
    "    # unflatten batch dimension and convert to numpy array \n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy() \n",
    "    \n",
    "    return {'loss': loss, 'predicted_label': predicted_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function forward_pass_with_label at 0x7fd9d0871280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_set = panx_hi_encoded[\"validation\"] \n",
    "valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\n",
    "df = valid_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 2218, 14136, 5988, 67691, 460, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]</td>\n",
       "      <td>[0.0, 0.03533609, 0.0, 0.027601829, 0.02570483...</td>\n",
       "      <td>[O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O]</td>\n",
       "      <td>[&lt;s&gt;, ▁स, जन, ▁घर, ▁जाना, ▁है, &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 93019, 7475, 976, 156711, 41612, 3558, 967...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, O, IGN, IGN, IGN, B-PER, IGN, IGN, IGN, ...</td>\n",
       "      <td>[0.0, 0.0056619984, 0.0, 0.0, 0.0, 0.019219365...</td>\n",
       "      <td>[O, O, O, O, O, B-PER, I-PER, I-PER, I-PER, I-...</td>\n",
       "      <td>[&lt;s&gt;, ▁पुनर्, प्र, े, षित, ▁फ़, ि, रो, ज़, ▁शा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 11026, 3849, 8389, 1471, 871, 76302, 659, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-LOC, IGN, IGN, IGN, I-LOC, I-LOC, IGN,...</td>\n",
       "      <td>[0.0, 0.07252373, 0.0, 0.0, 0.0, 0.05340195, 0...</td>\n",
       "      <td>[I-LOC, B-LOC, I-LOC, I-LOC, I-LOC, I-LOC, I-L...</td>\n",
       "      <td>[&lt;s&gt;, ▁ला, ह, ौ, ल, ▁और, ▁स्प, ी, ति, ▁जिला, &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 20571, 3282, 6, 150685, 20, 3813, 1187, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-PER, IGN, I-PER, IGN, O, O, IGN, IGN, ...</td>\n",
       "      <td>[0.0, 0.012027361, 0.0, 0.016277391, 0.0, 0.00...</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, I-PER, O, O, O, O, O]</td>\n",
       "      <td>[&lt;s&gt;, ▁अस, ित, ▁, सेन, ▁-, ▁था, ने, दार, &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 9163, 2629, 76183, 1472, 6, 4, 46005, 1187...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-LOC, IGN, IGN, IGN, I-LOC, IGN, I-LOC,...</td>\n",
       "      <td>[0.0, 0.055613037, 0.0, 0.0, 0.0, 1.1047724, 0...</td>\n",
       "      <td>[I-LOC, B-LOC, B-LOC, I-LOC, I-LOC, O, O, B-LO...</td>\n",
       "      <td>[&lt;s&gt;, ▁मे, म्, फि, स, ▁, ,, ▁टे, ने, सी, &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids   \n",
       "0              [0, 2218, 14136, 5988, 67691, 460, 2]  \\\n",
       "1  [0, 93019, 7475, 976, 156711, 41612, 3558, 967...   \n",
       "2  [0, 11026, 3849, 8389, 1471, 871, 76302, 659, ...   \n",
       "3  [0, 20571, 3282, 6, 150685, 20, 3813, 1187, 11...   \n",
       "4  [0, 9163, 2629, 76183, 1472, 6, 4, 46005, 1187...   \n",
       "\n",
       "                                     attention_mask   \n",
       "0                             [1, 1, 1, 1, 1, 1, 1]  \\\n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "2                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                              labels   \n",
       "0        [IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]  \\\n",
       "1  [IGN, O, IGN, IGN, IGN, B-PER, IGN, IGN, IGN, ...   \n",
       "2  [IGN, B-LOC, IGN, IGN, IGN, I-LOC, I-LOC, IGN,...   \n",
       "3  [IGN, B-PER, IGN, I-PER, IGN, O, O, IGN, IGN, ...   \n",
       "4  [IGN, B-LOC, IGN, IGN, IGN, I-LOC, IGN, I-LOC,...   \n",
       "\n",
       "                                                loss   \n",
       "0  [0.0, 0.03533609, 0.0, 0.027601829, 0.02570483...  \\\n",
       "1  [0.0, 0.0056619984, 0.0, 0.0, 0.0, 0.019219365...   \n",
       "2  [0.0, 0.07252373, 0.0, 0.0, 0.0, 0.05340195, 0...   \n",
       "3  [0.0, 0.012027361, 0.0, 0.016277391, 0.0, 0.00...   \n",
       "4  [0.0, 0.055613037, 0.0, 0.0, 0.0, 1.1047724, 0...   \n",
       "\n",
       "                                     predicted_label   \n",
       "0          [O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, O]  \\\n",
       "1  [O, O, O, O, O, B-PER, I-PER, I-PER, I-PER, I-...   \n",
       "2  [I-LOC, B-LOC, I-LOC, I-LOC, I-LOC, I-LOC, I-L...   \n",
       "3     [O, B-PER, I-PER, I-PER, I-PER, O, O, O, O, O]   \n",
       "4  [I-LOC, B-LOC, B-LOC, I-LOC, I-LOC, O, O, B-LO...   \n",
       "\n",
       "                                        input_tokens  \n",
       "0               [<s>, ▁स, जन, ▁घर, ▁जाना, ▁है, </s>]  \n",
       "1  [<s>, ▁पुनर्, प्र, े, षित, ▁फ़, ि, रो, ज़, ▁शा...  \n",
       "2  [<s>, ▁ला, ह, ौ, ल, ▁और, ▁स्प, ी, ति, ▁जिला, <...  \n",
       "3     [<s>, ▁अस, ित, ▁, सेन, ▁-, ▁था, ने, दार, </s>]  \n",
       "4     [<s>, ▁मे, म्, फि, स, ▁, ,, ▁टे, ने, सी, </s>]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2tag[-100] = \"IGN\"\n",
    "df[\"input_tokens\"] = df[\"input_ids\"].apply(\n",
    "    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n",
    "df[\"predicted_label\"] = df[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "df[\"labels\"] = df[\"labels\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x]) \n",
    "df[\"loss\"] = df.apply(\n",
    "    lambda x: x[\"loss\"][:len(x[\"input_ids\"])], axis=1)\n",
    "df[\"predicted_label\"] = df.apply(\n",
    "    lambda x: x[\"predicted_label\"][:len(x[\"input_ids\"])], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2218</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>0.04</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁स</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5988</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.03</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁घर</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67691</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.03</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁जाना</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>460</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.03</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁है</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93019</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.01</td>\n",
       "      <td>O</td>\n",
       "      <td>▁पुनर्</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41612</td>\n",
       "      <td>1</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>0.02</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>▁फ़</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51757</td>\n",
       "      <td>1</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.03</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>▁शाह</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_ids attention_mask labels  loss predicted_label input_tokens\n",
       "0      2218              1  B-ORG  0.04           B-ORG           ▁स\n",
       "0      5988              1  I-ORG  0.03           I-ORG          ▁घर\n",
       "0     67691              1  I-ORG  0.03           I-ORG        ▁जाना\n",
       "0       460              1  I-ORG  0.03           I-ORG          ▁है\n",
       "1     93019              1      O  0.01               O       ▁पुनर्\n",
       "1     41612              1  B-PER  0.02           B-PER          ▁फ़\n",
       "1     51757              1  I-PER  0.03           I-PER         ▁शाह"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens = df.apply(pd.Series.explode) \n",
    "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2) \n",
    "df_tokens.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_tokens</th>\n",
       "      <td>▁</td>\n",
       "      <td>▁)</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁का</td>\n",
       "      <td>▁सी</td>\n",
       "      <td>▁डी</td>\n",
       "      <td>▁क्रिकेट</td>\n",
       "      <td>▁राज्य</td>\n",
       "      <td>▁N</td>\n",
       "      <td>▁पाठ्यक्रम</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>235</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.07</td>\n",
       "      <td>5.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>74.51</td>\n",
       "      <td>33.85</td>\n",
       "      <td>32.66</td>\n",
       "      <td>12.56</td>\n",
       "      <td>9.11</td>\n",
       "      <td>8.21</td>\n",
       "      <td>7.71</td>\n",
       "      <td>6.71</td>\n",
       "      <td>6.07</td>\n",
       "      <td>5.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0      1      2      3     4     5         6       7     8   \n",
       "input_tokens      ▁     ▁)     ▁(    ▁का   ▁सी   ▁डी  ▁क्रिकेट  ▁राज्य    ▁N  \\\n",
       "count           235     80     80     39     7     6         4       9     1   \n",
       "mean           0.32   0.42   0.41   0.32   1.3  1.37      1.93    0.75  6.07   \n",
       "sum           74.51  33.85  32.66  12.56  9.11  8.21      7.71    6.71  6.07   \n",
       "\n",
       "                       9  \n",
       "input_tokens  ▁पाठ्यक्रम  \n",
       "count                  1  \n",
       "mean                5.71  \n",
       "sum                 5.71  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    " .agg([\"count\", \"mean\", \"sum\"])\n",
    " .droplevel(level=0, axis=1)\n",
    " .sort_values(by=\"sum\", ascending=False)\n",
    " .reset_index()\n",
    " .round(2)\n",
    " .head(10)\n",
    " .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>407</td>\n",
       "      <td>163</td>\n",
       "      <td>976</td>\n",
       "      <td>259</td>\n",
       "      <td>185</td>\n",
       "      <td>140</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>138.0</td>\n",
       "      <td>120.02</td>\n",
       "      <td>119.42</td>\n",
       "      <td>78.68</td>\n",
       "      <td>72.38</td>\n",
       "      <td>65.3</td>\n",
       "      <td>43.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2      3      4      5      6\n",
       "labels  I-ORG   I-LOC       O  I-PER  B-LOC  B-ORG  B-PER\n",
       "count     407     163     976    259    185    140    177\n",
       "mean     0.34    0.74    0.12    0.3   0.39   0.47   0.25\n",
       "sum     138.0  120.02  119.42  78.68  72.38   65.3  43.59"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"labels\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix \n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show() \n",
    "    \n",
    "plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"], tags.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁चिन</td>\n",
       "      <td>▁राज्य</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁प्राचीन</td>\n",
       "      <td>▁चीन</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>2.65</td>\n",
       "      <td>3.74</td>\n",
       "      <td>2.91</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.69</td>\n",
       "      <td>2.87</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1      2         3      4      5      6\n",
       "tokens   ▁चिन  ▁राज्य     ▁(  ▁प्राचीन   ▁चीन     ▁)   </s>\n",
       "labels  B-ORG   I-ORG  I-ORG     I-ORG  I-ORG  I-ORG    IGN\n",
       "preds   B-LOC   I-LOC  I-LOC     I-LOC  I-LOC  I-LOC  I-LOC\n",
       "losses   2.65    3.74   2.91      3.49   3.69   2.87   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁एन</td>\n",
       "      <td>एस</td>\n",
       "      <td>ई</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁N</td>\n",
       "      <td>SE</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>1.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.81</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1     2      3      4     5      6     7\n",
       "tokens    ▁एन    एस     ई     ▁(     ▁N    SE     ▁)  </s>\n",
       "labels  B-ORG   IGN   IGN  I-ORG  I-ORG   IGN  I-ORG   IGN\n",
       "preds       O     O     O      O      O     O      O     O\n",
       "losses   1.83  0.00  0.00   5.81   6.07  0.00   5.62  0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁</td>\n",
       "      <td>विद</td>\n",
       "      <td>र्</td>\n",
       "      <td>भ</td>\n",
       "      <td>▁क्रिकेट</td>\n",
       "      <td>▁एस</td>\n",
       "      <td>ोस</td>\n",
       "      <td>िए</td>\n",
       "      <td>शन</td>\n",
       "      <td>▁</td>\n",
       "      <td>ग्रा</td>\n",
       "      <td>उ</td>\n",
       "      <td>ंड</td>\n",
       "      <td>▁</td>\n",
       "      <td>,</td>\n",
       "      <td>▁नाग</td>\n",
       "      <td>पुर</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>3.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.73</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1      2      3         4      5      6      7      8    \n",
       "tokens      ▁    विद     र्      भ  ▁क्रिकेट    ▁एस     ोस     िए     शन  \\\n",
       "labels  B-LOC    IGN    IGN    IGN     I-LOC  I-LOC    IGN    IGN    IGN   \n",
       "preds   B-ORG  B-ORG  I-ORG  I-ORG     I-ORG  I-ORG  I-ORG  I-ORG  I-ORG   \n",
       "losses   3.21   0.00   0.00   0.00      4.73   4.71   0.00   0.00   0.00   \n",
       "\n",
       "           9      10     11     12     13     14     15     16    17  \n",
       "tokens      ▁   ग्रा      उ     ंड      ▁      ,   ▁नाग    पुर  </s>  \n",
       "labels  I-LOC    IGN    IGN    IGN      O    IGN  B-LOC    IGN   IGN  \n",
       "preds   I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  B-LOC  I-ORG     O  \n",
       "losses   3.86   0.00   0.00   0.00   1.10   0.00   0.36   0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_samples(df):\n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], [] \n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\":labels, \"preds\": preds, \"losses\": losses}).T\n",
    "\n",
    "        yield df_tmp\n",
    "\n",
    "df[\"total_loss\"] = df[\"loss\"].apply(sum)\n",
    "df_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁मिल</td>\n",
       "      <td>ो</td>\n",
       "      <td>रा</td>\n",
       "      <td>ड</td>\n",
       "      <td>▁म</td>\n",
       "      <td>ज़</td>\n",
       "      <td>िक</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁सर्</td>\n",
       "      <td>बिया</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>5.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1      2      3      4      5      6     7      8      9    \n",
       "tokens   ▁मिल      ो     रा      ड     ▁म     ज़     िक    ▁(   ▁सर्   बिया  \\\n",
       "labels      O    IGN    IGN    IGN      O    IGN    IGN     O  B-LOC    IGN   \n",
       "preds   B-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER     O  B-LOC  I-LOC   \n",
       "losses   5.18   0.00   0.00   0.00   4.01   0.00   0.00  0.55   0.43   0.00   \n",
       "\n",
       "           10    11  \n",
       "tokens     ▁)  </s>  \n",
       "labels      O   IGN  \n",
       "preds   I-PER     O  \n",
       "losses   0.95  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁राजेन्द्र</td>\n",
       "      <td>▁सिंह</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁र</td>\n",
       "      <td>ज्</td>\n",
       "      <td>जू</td>\n",
       "      <td>▁भै</td>\n",
       "      <td>या</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0      1      2      3      4      5      6      7      8   \n",
       "tokens  ▁राजेन्द्र  ▁सिंह     ▁(     ▁र     ज्     जू    ▁भै     या     ▁)  \\\n",
       "labels       B-PER  I-PER  I-PER  I-PER    IGN    IGN  I-PER    IGN  I-PER   \n",
       "preds        B-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER   \n",
       "losses        0.03   0.02   0.55   0.82   0.00   0.00   0.04   0.00   0.45   \n",
       "\n",
       "           9  \n",
       "tokens  </s>  \n",
       "labels   IGN  \n",
       "preds      O  \n",
       "losses  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Lingual Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_f1_score(trainer, dataset):\n",
    "    return trainer.predict(dataset).metrics[\"test_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [hi] model on [hi] dataset: 0.762\n"
     ]
    }
   ],
   "source": [
    "f1_scores = defaultdict(dict)\n",
    "f1_scores[\"hi\"][\"hi\"] = get_f1_score(trainer, panx_hi_encoded[\"test\"])\n",
    "print(f\"F1-score of [hi] model on [hi] dataset: {f1_scores['hi']['hi']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁నా</td>\n",
       "      <td>▁అమ్మ</td>\n",
       "      <td>▁క</td>\n",
       "      <td>ళ్ల</td>\n",
       "      <td>కు</td>\n",
       "      <td>▁నీ</td>\n",
       "      <td>ళ్ళు</td>\n",
       "      <td>▁</td>\n",
       "      <td>పోయాయి</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4   5    6     7  8       9     10\n",
       "Tokens  <s>    ▁నా  ▁అమ్మ     ▁క    ళ్ల  కు  ▁నీ  ళ్ళు  ▁  పోయాయి  </s>\n",
       "Tags      O  B-PER  I-PER  I-ORG  I-ORG   O    O     O  O       O     O"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_te = \"నా అమ్మ కళ్లకు నీళ్ళు పోయాయి\"\n",
    "tag_text(text_te, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lang_performance(lang, trainer):\n",
    "    panx_ds = encode_panx_dataset(panx_ch[lang])\n",
    "    return get_f1_score(trainer, panx_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/55 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0a9be84a8e490af0.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-7a1e4f78c3e1049a.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1533 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [hi] model pn [te] dataset: 0.517\n",
      "F1-score of [hi] model pn [ta] dataset: 0.544\n",
      "F1-score of [hi] model pn [en] dataset: 0.505\n"
     ]
    }
   ],
   "source": [
    "f1_scores[\"hi\"][\"te\"] = evaluate_lang_performance(\"te\", trainer)\n",
    "f1_scores[\"hi\"][\"ta\"] = evaluate_lang_performance(\"ta\", trainer)\n",
    "f1_scores[\"hi\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\n",
    "print(f\"F1-score of [hi] model pn [te] dataset: {f1_scores['hi']['te']:.3f}\")\n",
    "print(f\"F1-score of [hi] model pn [ta] dataset: {f1_scores['hi']['ta']:.3f}\")\n",
    "print(f\"F1-score of [hi] model pn [en] dataset: {f1_scores['hi']['en']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_on_subset(dataset, num_samples):\n",
    "    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples)) \n",
    "    valid_ds = dataset[\"validation\"]\n",
    "    test_ds = dataset[\"test\"]\n",
    "    training_args.logging_steps = len(train_ds) // batch_size \n",
    "    \n",
    "    trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                      data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                      train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n",
    "    trainer.train() \n",
    "    if training_args.push_to_hub:\n",
    "        trainer.push_to_hub(commit_message=\"Training completed!\") \n",
    "        \n",
    "    f1_score = get_f1_score(trainer, test_ds)\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0b1a043e5fbd9bc3.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-1251990df669ce8b.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-b75ff5749b911307.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-5eca1d3cb0a1cf30.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.323507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.739300</td>\n",
       "      <td>1.146330</td>\n",
       "      <td>0.013675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.739300</td>\n",
       "      <td>1.073598</td>\n",
       "      <td>0.022913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_samples</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.009852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_samples  f1_score\n",
       "0          250  0.009852"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_ta_encoded = encode_panx_dataset(panx_ch[\"ta\"])\n",
    "training_args.push_to_hub = False \n",
    "metrics_df = train_on_subset(panx_ta_encoded, 250)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-5eca1d3cb0a1cf30.arrow\n",
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.156404</td>\n",
       "      <td>0.006192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.466300</td>\n",
       "      <td>0.987687</td>\n",
       "      <td>0.137143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.466300</td>\n",
       "      <td>0.902659</td>\n",
       "      <td>0.164493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-5eca1d3cb0a1cf30.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.859298</td>\n",
       "      <td>0.179732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.092700</td>\n",
       "      <td>0.545415</td>\n",
       "      <td>0.525869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.092700</td>\n",
       "      <td>0.467190</td>\n",
       "      <td>0.572635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-5eca1d3cb0a1cf30.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.678037</td>\n",
       "      <td>0.438784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.349452</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.328346</td>\n",
       "      <td>0.710345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-5eca1d3cb0a1cf30.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 01:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.335882</td>\n",
       "      <td>0.674699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.522800</td>\n",
       "      <td>0.252491</td>\n",
       "      <td>0.751547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.522800</td>\n",
       "      <td>0.248702</td>\n",
       "      <td>0.774423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for num_samples in [500, 1000, 2000, 4000]:\n",
    "    result = train_on_subset(panx_ta_encoded, num_samples)\n",
    "    metrics_df = pd.concat([metrics_df, result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(f1_scores[\"hi\"][\"ta\"], ls=\"--\", color=\"r\")\n",
    "metrics_df.set_index(\"num_samples\").plot(ax=ax) \n",
    "plt.legend([\"Zero-shot from hi\", \"Fine-tuned on ta\"], loc=\"lower right\")\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel(\"Number of Training Samples\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning on Multiple Languages at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def concatenate_splits(corpora):\n",
    "    multi_corpus = DatasetDict()\n",
    "    for split in corpora[0].keys():\n",
    "        multi_corpus[split] = concatenate_datasets([corpus[split] for corpus in corpora]).shuffle(seed=42)\n",
    "    return multi_corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "panx_hi_ta_encoded = concatenate_splits([panx_hi_encoded, panx_ta_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-hi-ta into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [558/558 02:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.317362</td>\n",
       "      <td>0.767963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.391900</td>\n",
       "      <td>0.236424</td>\n",
       "      <td>0.806194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.391900</td>\n",
       "      <td>0.219470</td>\n",
       "      <td>0.838800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692015cf1b9d4a73a3a1d1232bd1b627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04a4a118c8241e8ac7ea32796cbd47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tokenizer.json:   0%|          | 1.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f182d74211854c21b0ff5c88c315cff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-hi-ta\n",
      "   686f941..eb18e8b  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-hi-ta\n",
      "   eb18e8b..529deb3  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-hi-ta/commit/eb18e8b770057b0656e39058767994d9bd50058b'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.logging_steps = len(panx_hi_ta_encoded[\"train\"]) // batch_size \n",
    "training_args.push_to_hub = True \n",
    "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-hi-ta\"\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args, \n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics, \n",
    "                  tokenizer=xlmr_tokenizer, train_dataset=panx_hi_ta_encoded[\"train\"],\n",
    "                  eval_dataset=panx_hi_ta_encoded[\"validation\"])\n",
    "\n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-77e21bc76f53307c.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0a9be84a8e490af0.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-7a1e4f78c3e1049a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [hi-fa] model on [hi] dataset: 0.817\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0b1a043e5fbd9bc3.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-1251990df669ce8b.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-b75ff5749b911307.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [hi-fa] model on [te] dataset: 0.577\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-26c09d9e10a651a1.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-722ed6e24115ceef.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-be4fd7323f8961aa.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [hi-fa] model on [ta] dataset: 0.798\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of [hi-fa] model on [en] dataset: 0.389\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    f1 = evaluate_lang_performance(lang, trainer)\n",
    "    print(f\"F1-score of [hi-fa] model on [{lang}] dataset: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-77e21bc76f53307c.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0a9be84a8e490af0.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.te/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-7a1e4f78c3e1049a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-te into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.342200</td>\n",
       "      <td>2.132988</td>\n",
       "      <td>0.070652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.953400</td>\n",
       "      <td>1.635856</td>\n",
       "      <td>0.036364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.729000</td>\n",
       "      <td>1.380958</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345b28a4fd034e07a4a92ab946e4040e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ae352b67940e89411fc6d9dc59aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4f1a680ac84fe3bd4c4c8adce67eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tokenizer.json:   0%|          | 1.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-te\n",
      "   83c8508..85cd866  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-te\n",
      "   85cd866..011be9d  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-0b1a043e5fbd9bc3.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-1251990df669ce8b.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-b75ff5749b911307.arrow\n",
      "Loading cached shuffled indices for dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.ta/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-5eca1d3cb0a1cf30.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-ta into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='429' max='429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [429/429 01:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285014</td>\n",
       "      <td>0.741187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>0.225861</td>\n",
       "      <td>0.778761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>0.222329</td>\n",
       "      <td>0.807520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6432bdf7df9c4bc3aeac47def626a187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a658409d0fc043cb85f31df919d118a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tokenizer.json:   0%|          | 1.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ef51a517d44f428be61c2d5e682754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-ta\n",
      "   0b7047d..bfd21e5  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-ta\n",
      "   bfd21e5..a0288f6  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-26c09d9e10a651a1.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-722ed6e24115ceef.arrow\n",
      "Loading cached processed dataset at /u/akommala/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/29f5d57a48779f37ccb75cb8708d1095448aad0713b425bdc1ff9a4a128a56e4/cache-be4fd7323f8961aa.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-en into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.651563</td>\n",
       "      <td>0.501952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.428605</td>\n",
       "      <td>0.647899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.406697</td>\n",
       "      <td>0.666099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a0fc66d2314c06b9c909e0ceb654b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885f93d60bc34f09bcc3731c1ffd9b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tokenizer.json:   0%|          | 1.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c503d9eb44f48ac82da29ef6e176f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-en\n",
      "   c6a517b..a0bd435  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-en\n",
      "   a0bd435..8306f52  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpora = [panx_hi_encoded] \n",
    "\n",
    "# exclude hindi from iteration\n",
    "for lang in langs[1:]:\n",
    "    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n",
    "    # fine-tune on monolingual corpus \n",
    "    ds_encoded = encode_panx_dataset(panx_ch[lang])\n",
    "    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n",
    "    # collect F1-scores in common dict \n",
    "    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n",
    "    # add monolingual corpus to list of corpora to concatenate\n",
    "    corpora.append(ds_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpora_encoded = concatenate_splits(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-all into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='657' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [657/657 02:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.329411</td>\n",
       "      <td>0.719106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>0.303813</td>\n",
       "      <td>0.758561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>0.292166</td>\n",
       "      <td>0.780616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50016fa95b6489cb757582d9afd7da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1f4e8cd1e64831875b07da2951188e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tokenizer.json:   0%|          | 1.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72846ff0e1064073bd9f05c001373ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-all\n",
      "   5beabd0..bf50281  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-all\n",
      "   bf50281..d92ec7e  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/akommala/xlm-roberta-base-finetuned-panx-all/commit/bf5028133d5edd94a2553e79a72d7a9d594bce35'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size \n",
    "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n",
    "                  eval_dataset=corpora_encoded[\"validation\"])\n",
    "\n",
    "trainer.train()\n",
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/akommala/nlp-with-transformers/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluated on</th>\n",
       "      <th>hi</th>\n",
       "      <th>te</th>\n",
       "      <th>ta</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine-tune on</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>0.7615</td>\n",
       "      <td>0.5170</td>\n",
       "      <td>0.5441</td>\n",
       "      <td>0.5049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>each</th>\n",
       "      <td>0.7615</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.7742</td>\n",
       "      <td>0.6812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.6099</td>\n",
       "      <td>0.8109</td>\n",
       "      <td>0.7442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluated on      hi      te      ta      en\n",
       "Fine-tune on                                \n",
       "hi            0.7615  0.5170  0.5441  0.5049\n",
       "each          0.7615  0.0270  0.7742  0.6812\n",
       "all           0.8032  0.6099  0.8109  0.7442"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, lang in enumerate(langs):\n",
    "    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])\n",
    "scores_data = {\"hi\": f1_scores[\"hi\"],\n",
    "               \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n",
    "               \"all\": f1_scores[\"all\"]}\n",
    "f1_scores_df = pd.DataFrame(scores_data).T.round(4)\n",
    "f1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\", \n",
    "                         inplace=True)\n",
    "f1_scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
